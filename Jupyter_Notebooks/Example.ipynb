{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example for Paper**: [Non-Euclidean Universal Approximation](https://arxiv.org/abs/2006.02341)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode:\n",
    "Use this to test script before running with \"train_mode\" $\\triangleq$ False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 0.3\n",
    "min_height = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Hyperparameter_Grid.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "exec(open('Helper_Utility.py').read())\n",
    "exec(open('Optimal_Deep_Feature_and_Readout_Util.py').read())\n",
    "# Pre-process Data\n",
    "exec(open('Prepare_Data_California_Housing.py').read())\n",
    "# Import time separately\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing\n",
    "\n",
    "We compare three models in this implementation.  Each are feed-forward networks of the same dimensions:\n",
    "- **Good model**: repsects our assumptions\n",
    "- **Bad model**: does not\n",
    "- **Vanilla model**: is a naive feed-forward benchmark\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Seed(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed Tensorflow:\n",
    "tf.random.set_seed(2020)\n",
    "# Set seed Numpy:\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $I$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  \n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Mode: <Good I>\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Nice_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Nice_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   4 | elapsed:    8.3s remaining:    8.3s\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:   17.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:   17.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7067 - mse: 1.0185 - mae: 0.7067 - mape: 36.9670\n",
      "1806/1806 [==============================] - 1s 821us/step\n",
      "774/774 [==============================] - 1s 808us/step\n",
      "Cross-Validated Model: <Good I>\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_good, y_hat_test_good = build_and_predict_nice_model(n_folds = n_jobs, n_jobs = n_jobs)\n",
    "print('Cross-Validated Model: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $II$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ (x,\\phi_{\\operatorname{Random}}(x)):\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi_{\\operatorname{Random}}(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$,\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices, and $A_i,b_i$ are generated randomly by drawing their components from the standardized Bernoulli distribution.\n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Deep Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step: 0.25\n",
      "Current Step: 0.5\n",
      "Current Step: 0.75\n",
      "Current Step: 1.0\n",
      "Generated Features: Done!\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.227733  0.516472  0.431373  0.013022  0.010147  0.033712  0.184391   \n",
      "1  0.510121  0.198725  0.411765  0.066382  0.038008  0.072028  0.250479   \n",
      "2  0.462551  0.206164  0.392157  0.060405  0.028085  0.053774  0.512055   \n",
      "3  0.648785  0.133900  0.529412  0.074063  0.041904  0.081894  0.306410   \n",
      "4  0.344130  0.537726  0.666667  0.037922  0.020293  0.043907  0.171184   \n",
      "\n",
      "         7         8         9   ...        5         6         7         8   \\\n",
      "0  0.330255  0.488221  0.347356  ...  0.033712  0.184391  0.330255  0.488221   \n",
      "1  0.311605  0.603722  0.448411  ...  0.072028  0.250479  0.311605  0.603722   \n",
      "2  0.240544  0.695158  0.478714  ...  0.053774  0.512055  0.240544  0.695158   \n",
      "3  0.561180  0.469450  0.532105  ...  0.081894  0.306410  0.561180  0.469450   \n",
      "4  0.562207  0.324437  0.385467  ...  0.043907  0.171184  0.562207  0.324437   \n",
      "\n",
      "         9    10   11   12   13   14  \n",
      "0  0.347356  0.0  0.0  0.0  1.0  0.0  \n",
      "1  0.448411  0.0  0.0  0.0  0.0  1.0  \n",
      "2  0.478714  0.0  0.0  0.0  0.0  1.0  \n",
      "3  0.532105  1.0  0.0  0.0  0.0  0.0  \n",
      "4  0.385467  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.607287  0.165781  0.294118  0.074470  0.046052  0.160993  0.147991   \n",
      "1  0.167004  0.583422  0.803922  0.035811  0.017350  0.047854  0.250893   \n",
      "2  0.314777  0.569607  0.294118  0.001221  0.000420  0.000822  0.146557   \n",
      "3  0.220648  0.521785  0.666667  0.026019  0.010735  0.037987  0.324106   \n",
      "4  0.728745  0.023379  0.764706  0.045959  0.023012  0.074823  0.148405   \n",
      "\n",
      "         7         8         9   ...        5         6         7         8   \\\n",
      "0  0.488925  0.472932  0.471865  ...  0.160993  0.147991  0.488925  0.472932   \n",
      "1  0.305463  0.663587  0.503898  ...  0.047854  0.250893  0.305463  0.663587   \n",
      "2  0.519729  0.405348  0.430294  ...  0.000822  0.146557  0.519729  0.405348   \n",
      "3  0.321046  0.507506  0.358849  ...  0.037987  0.324106  0.321046  0.507506   \n",
      "4  0.505424  0.658622  0.675360  ...  0.074823  0.148405  0.505424  0.658622   \n",
      "\n",
      "         9    10   11   12   13   14  \n",
      "0  0.471865  1.0  0.0  0.0  0.0  0.0  \n",
      "1  0.503898  0.0  0.0  0.0  0.0  1.0  \n",
      "2  0.430294  0.0  1.0  0.0  0.0  0.0  \n",
      "3  0.358849  0.0  0.0  0.0  1.0  0.0  \n",
      "4  0.675360  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "### Initialize Parameters\n",
    "#------------------------#\n",
    "# Initialize History\n",
    "Randomized_Depth = np.random.poisson(2)\n",
    "past_val = -1\n",
    "current_position = 0\n",
    "# Initalize Features\n",
    "X_train_features = X_train\n",
    "X_test_features = X_test\n",
    "\n",
    "# Construct Deep Randomized Features\n",
    "#------------------------------------#\n",
    "# Set Seed\n",
    "np.random.seed(2020)\n",
    "\n",
    "\n",
    "# Builds Features\n",
    "for i in range(N_Features):    \n",
    "    # Transformations\n",
    "    #-----------------#\n",
    "    # Build\n",
    "    if Randomized_Depth > 0:\n",
    "        # Note: Write Non-Liearly Transformed Features only if transformation has been applied, only if Depth >0\n",
    "        \n",
    "        # Apply Activation\n",
    "        X_train_features_loop = compositer(X_train_features)\n",
    "        X_test_features_loop = compositer(X_test_features)\n",
    "        # Apply Random Weights\n",
    "        Weights_random = (np.random.binomial(1,.5,(X_train_features_loop.shape[1],X_train_features_loop.shape[1])) - .5)*2 # Generate Random Weights\n",
    "        X_train_features_loop = np.matmul(X_train_features_loop,Weights_random)\n",
    "        X_test_features_loop = np.matmul(X_test_features_loop,Weights_random)\n",
    "        # # Apply Bias\n",
    "        biases_random = (np.random.binomial(1,.5,X_train_features_loop.shape[1]) -.5)*2         # Generate Random Weights and Biases from Recentered Binomial Law\n",
    "        X_train_features_loop = X_train_features_loop + biases_random\n",
    "        X_test_features_loop = X_test_features_loop + biases_random\n",
    "        \n",
    "    else:\n",
    "        X_train_features_loop = X_train_features\n",
    "        X_test_features_loop = X_test_features\n",
    "\n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print(\"Current Step: \" +str((i+1)/N_Features))\n",
    "    \n",
    "# Coerce into nice form:\n",
    "X_train_features = pd.DataFrame(X_train_features)\n",
    "X_test_features = pd.DataFrame(X_test_features)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_train_features.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "X_test_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create Features\n",
    "Random_Feature_Space_train = pd.concat([X_train,X_train_features],axis=1)\n",
    "Random_Feature_Space_test = pd.concat([X_test,X_test_features],axis=1)\n",
    "\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Generated Features: Done!')\n",
    "print(Random_Feature_Space_train.head())\n",
    "print(Random_Feature_Space_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Mode: <Good II>\n"
     ]
    }
   ],
   "source": [
    "# Reload Grid\n",
    "exec(open('Hyperparameter_Grid.py').read())\n",
    "# Adjust Input Space's Dimension\n",
    "param_grid_Nice_Nets['input_dim'] = [Random_Feature_Space_train.shape[1]]\n",
    "\n",
    "def def_trainable_layers_Randomized_Feature(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "#     for i_depth_readout in range(Depth_Readout_Map):\n",
    "#         # First Layer\n",
    "#         if i_depth_readout == 0:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "#         else:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_modelII(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Randomized_Feature, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(Random_Feature_Space_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(Random_Feature_Space_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(Random_Feature_Space_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good II>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   4 | elapsed:    3.8s remaining:    3.8s\n",
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    9.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    9.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806/1806 [==============================] - 1s 696us/step - loss: 0.8159 - mse: 1.3526 - mae: 0.8159 - mape: 41.7495\n",
      "1806/1806 [==============================] - 1s 511us/step\n",
      "774/774 [==============================] - 0s 522us/step\n",
      "Cross-Validated Model: \"Good II\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_goodII, y_hat_test_goodII = build_and_predict_nice_modelII(n_folds = n_folds, n_jobs = n_jobs)\n",
    "print('Cross-Validated Model: \"Good II\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Benchmark(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload CV Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('Hyperparameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Model:\n",
    "Build and train the *bad* model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  The maps $\\rho$ and $\\phi$ are neither injective nor are they surjective since $x\\mapsto \\operatorname{ReLU}(x)$ is neither injective nor surjective as a map from $\\mathbb{R}^k$ to $\\mathbb{R}^k$; where $m=n$.  \n",
    "\n",
    "*Note*:  The key point here is that the input and output maps are forced to be of the same dimension.  Note that, this also violates the minimal bounds derivated in [this paper](https://arxiv.org/abs/1710.11278) for deep ReLU networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Bad Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Bad_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim,output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_bad_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Bad_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Bad_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Bad_Model_CVer = RandomizedSearchCV(estimator=Bad_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Bad_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Bad_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Bad_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Bad Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   4 | elapsed:    4.0s remaining:    4.0s\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    8.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    8.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806/1806 [==============================] - 2s 932us/step - loss: 0.8040 - mse: 1.3051 - mae: 0.8040 - mape: 42.0007\n",
      "1806/1806 [==============================] - 1s 571us/step\n",
      "774/774 [==============================] - 0s 530us/step\n",
      "Cross-Validated: Bad Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_bad, y_hat_test_bad = build_and_predict_bad_model(n_folds = n_iter, n_jobs = n_jobs)\n",
    "print('Cross-Validated: Bad Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark ffNN Model (Vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Vanilla(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_Vanilla_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Vanilla_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Vanilla, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Vanilla_Model_CVer = RandomizedSearchCV(estimator=Vanilla_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Vanilla_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Vanilla_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Vanilla_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 1 is smaller than n_iter=20. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   4 | elapsed:    3.9s remaining:    3.9s\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    8.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    8.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806/1806 [==============================] - 1s 715us/step - loss: 1.0968 - mse: 2.2574 - mae: 1.0968 - mape: 53.2523\n",
      "1806/1806 [==============================] - 1s 519us/step\n",
      "774/774 [==============================] - 0s 518us/step\n",
      "Cross-Validated: Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_Vanilla, y_hat_test_Vanilla = build_and_predict_Vanilla_model(n_folds = n_jobs, n_jobs = n_jobs)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Predictions/ Comparisons\n",
    "Generate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Models #\n",
    "#------------------#\n",
    "# Results with Good I Model\n",
    "Perform_GoodI = reporter(y_hat_train_good,y_hat_test_good,y_train,y_test)\n",
    "# Results with Good II Model\n",
    "Perform_GoodII = reporter(y_hat_train_goodII,y_hat_test_goodII,y_train,y_test)\n",
    "\n",
    "\n",
    "# Benchmark Models Performance #\n",
    "#------------------------------#\n",
    "# Results with Bad Model\n",
    "Perform_Bad = reporter(y_hat_train_bad,y_hat_test_bad,y_train,y_test)\n",
    "# Results Vanilla\n",
    "Perform_Vanilla = reporter(y_hat_train_Vanilla,y_hat_test_Vanilla,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_train = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.train,\n",
    "                    'Good II': Perform_GoodII.train,\n",
    "                    'Bad': Perform_Bad.train,\n",
    "                    'Vanilla': Perform_Vanilla.train})\n",
    "\n",
    "performance_test = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.test,\n",
    "                    'Good II': Perform_GoodII.test,\n",
    "                    'Bad': Perform_Bad.test,\n",
    "                    'Vanilla': Perform_Vanilla.test})\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_train.to_latex('./outputs/results/Performance_train.txt')\n",
    "performance_test.to_latex('./outputs/results/Performance_test.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_train_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_train))\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_test_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et-Voila!\n",
      " \n",
      " \n",
      "#-------------------#\n",
      " PERFORMANCE SUMMARY:\n",
      "#-------------------#\n",
      " \n",
      " \n",
      "---------------------\n",
      "Training Performance\n",
      "---------------------\n",
      "-------------------------------------------------------------\n",
      "         Good I    Good II        Bad    Vanilla\n",
      "MAE    0.556889   0.580294   0.585659   0.684897\n",
      "MSE    0.628276   0.666298   0.663276   0.895864\n",
      "MAPE  33.637338  29.717729  29.883210  35.282778\n",
      "-------------------------------------------------------------\n",
      "---------------------\n",
      "Testing Performance\n",
      "---------------------\n",
      "-------------------------------------------------------------\n",
      "         Good I    Good II        Bad    Vanilla\n",
      "MAE    0.552983   0.576258   0.581560   0.673208\n",
      "MSE    0.607161   0.643340   0.641471   0.856763\n",
      "MAPE  34.342083  29.809818  29.937939  34.915336\n",
      "-------------------------------------------------------------\n",
      " \n",
      " \n",
      "ðŸ˜ŠðŸ˜Š Fin ðŸ˜ŠðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "print('Et-Voila!')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('#-------------------#')\n",
    "print(' PERFORMANCE SUMMARY:')\n",
    "print('#-------------------#')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('---------------------')\n",
    "print('Training Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_train)\n",
    "print('-------------------------------------------------------------')\n",
    "print('---------------------')\n",
    "print('Testing Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_test)\n",
    "print('-------------------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('ðŸ˜ŠðŸ˜Š Fin ðŸ˜ŠðŸ˜Š')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ðŸ˜Š Fin ðŸ˜Š\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
