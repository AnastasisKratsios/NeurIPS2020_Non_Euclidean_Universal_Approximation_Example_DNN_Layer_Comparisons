{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example for Paper**: [Non-Euclidean Universal Approximation](https://arxiv.org/abs/2006.02341)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preping\n",
    "\n",
    "We compare three models in this implementation.  Each are feed-forward networks of the same dimensions:\n",
    "- **Good model**: repsects our assumptions\n",
    "- **Bad model**: does not\n",
    "- **Vanilla model**: is a naive feed-forward benchmark\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alert(s)\n",
    "import smtplib\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# DL: Tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.utils.layer_utils import count_params\n",
    "from tensorflow.python.framework import ops # Custome Tensorflow Functions\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "# DL: Tensorflow - Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Formatting:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "# Random Forest & Gradient Boosting (Arch. Construction)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Structuring\n",
    "from pathlib import Path\n",
    "\n",
    "# Visulatization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Writing, Reading, Exporting, and Importing\n",
    "#from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Misc\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "\n",
    "# Set-Seed\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Externally-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Utility\n",
    "exec(open('Helper_Utility.py').read())\n",
    "# Helper Functions Utility\n",
    "exec(open('Optimal_Deep_Feature_and_Readout_Util.py').read())\n",
    "# Extra Utilities\n",
    "exec(open('Grid_Enhanced_Network.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = \"./data/housing_complete.csv\"\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Parse/Prepare Data\n",
    "X_train, y_train, X_test, y_test= prepare_data(data_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and Make Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('./outputs/models/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Vanilla/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Bad/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/Good_I/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/Good_II/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/tables/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/results/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $I$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  \n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Mode: <Good I>\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Nice_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Nice_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   16.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "452/452 [==============================] - 3s 6ms/step - loss: 1.4207 - mse: 3.4930 - mae: 1.4207 - mape: 69.3144\n",
      "Epoch 2/2\n",
      "452/452 [==============================] - 2s 4ms/step - loss: 1.0788 - mse: 2.2514 - mae: 1.0788 - mape: 54.0674\n",
      "452/452 [==============================] - 1s 2ms/step\n",
      "194/194 [==============================] - 0s 2ms/step\n",
      "Cross-Validated Model: <Good I>\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_good, y_hat_test_good = build_and_predict_nice_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated Model: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $II$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = (x,\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1))$,\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices, and $A_i,b_i$ are generated randomly by drawing their components from the standardized Bernoulli distribution.\n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Deep Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step: 0.25\n",
      "Current Step: 0.5\n",
      "Current Step: 0.75\n",
      "Current Step: 1.0\n",
      "Generated Features: Done!\n",
      "   longitude  latitude  housing_median_age  total_rooms  population  \\\n",
      "0    -122.28     37.90                52.0       2261.0       819.0   \n",
      "1    -122.28     37.75                20.0       1156.0       583.0   \n",
      "2    -122.35     37.93                41.0        268.0       198.0   \n",
      "3    -122.44     37.76                30.0       5089.0      1935.0   \n",
      "4    -122.46     37.76                28.0       1072.0       363.0   \n",
      "\n",
      "   households  median_income        x1        x2        x3  longitude  \\\n",
      "0       335.0         4.9083  0.004823  0.020925  0.097437    -122.28   \n",
      "1       326.0         3.1972 -0.010159  0.017270 -0.051777    -122.28   \n",
      "2        82.0         3.2222 -0.009778  0.087640  0.127816    -122.35   \n",
      "3      1139.0         4.6053 -0.050311  0.171876 -0.040424    -122.44   \n",
      "4       168.0         6.1636 -0.055439  0.191173 -0.040088    -122.46   \n",
      "\n",
      "   latitude  housing_median_age  total_rooms  population  households  \\\n",
      "0     37.90                52.0       2261.0       819.0       335.0   \n",
      "1     37.75                20.0       1156.0       583.0       326.0   \n",
      "2     37.93                41.0        268.0       198.0        82.0   \n",
      "3     37.76                30.0       5089.0      1935.0      1139.0   \n",
      "4     37.76                28.0       1072.0       363.0       168.0   \n",
      "\n",
      "   median_income        x1        x2        x3  \n",
      "0         4.9083  0.004823  0.020925  0.097437  \n",
      "1         3.1972 -0.010159  0.017270 -0.051777  \n",
      "2         3.2222 -0.009778  0.087640  0.127816  \n",
      "3         4.6053 -0.050311  0.171876 -0.040424  \n",
      "4         6.1636 -0.055439  0.191173 -0.040088  \n",
      "   longitude  latitude  housing_median_age  total_rooms  population  \\\n",
      "0    -122.19     37.74                36.0        847.0       567.0   \n",
      "1    -122.49     38.10                43.0       1226.0       491.0   \n",
      "2    -122.26     37.81                34.0       5871.0      2689.0   \n",
      "3    -122.09     37.67                33.0       2431.0      1854.0   \n",
      "4    -122.12     37.65                26.0        162.0        86.0   \n",
      "\n",
      "   households  median_income        x1        x2        x3  longitude  \\\n",
      "0       159.0         1.1765  0.012122 -0.069903 -0.061536    -122.19   \n",
      "1       205.0         4.9286 -0.025284  0.214946  0.301317    -122.49   \n",
      "2      1789.0         2.8406  0.000961 -0.000437  0.007889    -122.26   \n",
      "3       603.0         2.7019  0.031197 -0.168769 -0.130512    -122.09   \n",
      "4        25.0         2.3750  0.021410 -0.140280 -0.150747    -122.12   \n",
      "\n",
      "   latitude  housing_median_age  total_rooms  population  households  \\\n",
      "0     37.74                36.0        847.0       567.0       159.0   \n",
      "1     38.10                43.0       1226.0       491.0       205.0   \n",
      "2     37.81                34.0       5871.0      2689.0      1789.0   \n",
      "3     37.67                33.0       2431.0      1854.0       603.0   \n",
      "4     37.65                26.0        162.0        86.0        25.0   \n",
      "\n",
      "   median_income        x1        x2        x3  \n",
      "0         1.1765  0.012122 -0.069903 -0.061536  \n",
      "1         4.9286 -0.025284  0.214946  0.301317  \n",
      "2         2.8406  0.000961 -0.000437  0.007889  \n",
      "3         2.7019  0.031197 -0.168769 -0.130512  \n",
      "4         2.3750  0.021410 -0.140280 -0.150747  \n"
     ]
    }
   ],
   "source": [
    "### Initialize Parameters\n",
    "#------------------------#\n",
    "# Initialize History\n",
    "Randomized_Depth = np.random.poisson(2)\n",
    "past_val = -1\n",
    "current_position = 0\n",
    "# Initalize Features\n",
    "X_train_features = X_train\n",
    "X_test_features = X_test\n",
    "\n",
    "# Construct Deep Randomized Features\n",
    "#------------------------------------#\n",
    "# Set Seed\n",
    "np.random.seed(2020)\n",
    "\n",
    "\n",
    "# Builds Features\n",
    "for i in range(N_Features):    \n",
    "    # Transformations\n",
    "    #-----------------#\n",
    "    # Build\n",
    "    if Randomized_Depth > 0:\n",
    "        # Note: Write Non-Liearly Transformed Features only if transformation has been applied, only if Depth >0\n",
    "        \n",
    "        # Apply Activation\n",
    "        X_train_features_loop = compositer(X_train_features)\n",
    "        X_test_features_loop = compositer(X_test_features)\n",
    "        # Apply Random Weights\n",
    "        Weights_random = (np.random.binomial(1,.5,(X_train_features_loop.shape[1],X_train_features_loop.shape[1])) - .5)*2 # Generate Random Weights\n",
    "        X_train_features_loop = np.matmul(X_train_features_loop,Weights_random)\n",
    "        X_test_features_loop = np.matmul(X_test_features_loop,Weights_random)\n",
    "        # # Apply Bias\n",
    "        biases_random = (np.random.binomial(1,.5,X_train_features_loop.shape[1]) -.5)*2         # Generate Random Weights and Biases from Recentered Binomial Law\n",
    "        X_train_features_loop = X_train_features_loop + biases_random\n",
    "        X_test_features_loop = X_test_features_loop + biases_random\n",
    "        \n",
    "    else:\n",
    "        X_train_features_loop = X_train_features\n",
    "        X_test_features_loop = X_test_features\n",
    "\n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print(\"Current Step: \" +str((i+1)/N_Features))\n",
    "    \n",
    "# Coerce into nice form:\n",
    "X_train_features = pd.DataFrame(X_train_features)\n",
    "X_test_features = pd.DataFrame(X_test_features)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_train_features.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "X_test_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create Features\n",
    "Random_Feature_Space_train = pd.concat([X_train,X_train_features],axis=1)\n",
    "Random_Feature_Space_test = pd.concat([X_test,X_test_features],axis=1)\n",
    "\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Generated Features: Done!')\n",
    "print(Random_Feature_Space_train.head())\n",
    "print(Random_Feature_Space_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Mode: <Good II>\n"
     ]
    }
   ],
   "source": [
    "# Reload Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Adjust Input Space's Dimension\n",
    "param_grid_Nice_Nets['input_dim'] = [Random_Feature_Space_train.shape[1]]\n",
    "\n",
    "def def_trainable_layers_Randomized_Feature(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "#     for i_depth_readout in range(Depth_Readout_Map):\n",
    "#         # First Layer\n",
    "#         if i_depth_readout == 0:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "#         else:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_modelII(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Randomized_Feature, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(Random_Feature_Space_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(Random_Feature_Space_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(Random_Feature_Space_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good II>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    6.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    6.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "452/452 [==============================] - 1s 1ms/step - loss: 2.0939 - mse: 5.7222 - mae: 2.0939 - mape: 101.3458\n",
      "Epoch 2/2\n",
      "452/452 [==============================] - 1s 1ms/step - loss: 2.0487 - mse: 5.5350 - mae: 2.0487 - mape: 98.3395\n",
      "452/452 [==============================] - 0s 692us/step\n",
      "194/194 [==============================] - 0s 726us/step\n",
      "Cross-Validated Model: \"Good II\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_goodII, y_hat_test_goodII = build_and_predict_nice_modelII(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated Model: \"Good II\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Benchmark(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload CV Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('Grid_Enhanced_Network.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Model:\n",
    "Build and train the *bad* model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  The maps $\\rho$ and $\\phi$ are neither injective nor are they surjective since $x\\mapsto \\operatorname{ReLU}(x)$ is neither injective nor surjective as a map from $\\mathbb{R}^k$ to $\\mathbb{R}^k$; where $m=n$.  \n",
    "\n",
    "*Note*:  The key point here is that the input and output maps are forced to be of the same dimension.  Note that, this also violates the minimal bounds derivated in [this paper](https://arxiv.org/abs/1710.11278) for deep ReLU networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Bad Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Bad_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim,output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_bad_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Bad_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Bad_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Bad_Model_CVer = RandomizedSearchCV(estimator=Bad_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Bad_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Bad_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Bad_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Bad Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "452/452 [==============================] - 1s 1ms/step - loss: 1.9985 - mse: 5.3318 - mae: 1.9985 - mape: 95.0137\n",
      "Epoch 2/2\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.9416 - mse: 5.1071 - mae: 1.9416 - mape: 91.2360\n",
      "452/452 [==============================] - 0s 666us/step\n",
      "194/194 [==============================] - 0s 662us/step\n",
      "Cross-Validated: Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_bad, y_hat_test_bad = build_and_predict_bad_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark ffNN Model (Vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Vanilla(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_Vanilla_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Vanilla_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Vanilla, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Vanilla_Model_CVer = RandomizedSearchCV(estimator=Vanilla_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Vanilla_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Vanilla_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Vanilla_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    2.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    2.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "452/452 [==============================] - 0s 946us/step - loss: 2.0052 - mse: 5.3588 - mae: 2.0052 - mape: 95.4514\n",
      "Epoch 2/2\n",
      "452/452 [==============================] - 0s 888us/step - loss: 1.9600 - mse: 5.1789 - mae: 1.9600 - mape: 92.4567\n",
      "452/452 [==============================] - 0s 584us/step\n",
      "194/194 [==============================] - 0s 603us/step\n",
      "Cross-Validated: Vanilla Model\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_Vanilla, y_hat_test_Vanilla = build_and_predict_Vanilla_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Predictions/ Comparisons\n",
    "Generate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Models #\n",
    "#------------------#\n",
    "# Results with Good I Model\n",
    "Perform_GoodI = reporter(y_hat_train_good,y_hat_test_good,y_train,y_test)\n",
    "# Results with Good II Model\n",
    "Perform_GoodII = reporter(y_hat_train_goodI,y_hat_test_goodII,y_train,y_test)\n",
    "\n",
    "\n",
    "# Benchmark Models Performance #\n",
    "#------------------------------#\n",
    "# Results with Bad Model\n",
    "Perform_Bad = reporter(y_hat_train_bad,y_hat_test_bad,y_train,y_test)\n",
    "# Results Vanilla\n",
    "Perform_Vanilla = reporter(y_hat_train_Vanilla,y_hat_test_Vanilla,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_train = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.train,\n",
    "                    'Good II': Perform_GoodII.train,\n",
    "                    'Bad': Perform_Bad.train,\n",
    "                    'Vanilla': Perform_Vanilla.train})\n",
    "\n",
    "performance_test = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.test,\n",
    "                    'Good II': Perform_GoodII.test,\n",
    "                    'Bad': Perform_Bad.test,\n",
    "                    'Vanilla': Perform_Vanilla.test})\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_train.to_latex('./outputs/results/Performance_train.txt')\n",
    "performance_test.to_latex('./outputs/results/Performance_test.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_train_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_train))\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_test_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et-Voila!\n",
      " \n",
      " \n",
      "#-------------------#\n",
      " PERFORMANCE SUMMARY:\n",
      "#-------------------#\n",
      " \n",
      " \n",
      "---------------------\n",
      "Training Performance\n",
      "---------------------\n",
      "-------------------------------------------------------------\n",
      "         Good I      Good II          Bad      Vanilla\n",
      "MAE    1.021502     1.931493     1.909398     1.937359\n",
      "MSE    2.002546     5.068936     4.982967     5.090549\n",
      "MAPE  77.900601  1353.623117  1162.452322  1421.507214\n",
      "-------------------------------------------------------------\n",
      "---------------------\n",
      "Testing Performance\n",
      "---------------------\n",
      "-------------------------------------------------------------\n",
      "         Good I      Good II          Bad      Vanilla\n",
      "MAE    1.012621     2.009092     1.892434     1.920396\n",
      "MSE    1.992302     5.354643     4.899480     5.006113\n",
      "MAPE  76.746722  4221.398045  1152.124826  1409.061450\n",
      "-------------------------------------------------------------\n",
      " \n",
      " \n",
      "Fin\n"
     ]
    }
   ],
   "source": [
    "print('Et-Voila!')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('#-------------------#')\n",
    "print(' PERFORMANCE SUMMARY:')\n",
    "print('#-------------------#')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('---------------------')\n",
    "print('Training Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_train)\n",
    "print('-------------------------------------------------------------')\n",
    "print('---------------------')\n",
    "print('Testing Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_test)\n",
    "print('-------------------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('Fin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 😊 Fin 😊\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
