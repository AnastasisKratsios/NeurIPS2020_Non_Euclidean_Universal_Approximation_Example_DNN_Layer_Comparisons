{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Transfer Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "This is a fork from the Architopes project's code:\n",
    "- CODE: https://github.com/bzamanlooy/Architopes\n",
    "- Paper: https://arxiv.org/abs/2006.14378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import Dependancies\n",
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Alert(s)\n",
    "import smtplib\n",
    "\n",
    "# DL: Tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.utils.layer_utils import count_params\n",
    "# DL: Tensorflow - Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import backend as K\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Formatting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Post-Processing: CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Timing\n",
    "from time import process_time, time\n",
    "\n",
    "# Misc\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Behnoosh\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLOCK GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CV Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This contains the CV-Grid\n",
    "from Grid import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Writing Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def check_file(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        raise FileExistsError(str(file_path) + \" does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data (Pre-)Processing Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_spherical_to_euclidean(df_input):\n",
    "    # First Create Spherical Coordinates From DataFrame\n",
    "    x1 = np.cos(df_input.latitude) * np.cos(df_input.longitude)\n",
    "    x2 = np.cos(df_input.latitude) * np.sin(df_input.longitude)\n",
    "    x3 = np.sin(df_input.latitude)\n",
    "    # Write to Data-Frame\n",
    "    coordinates = {\"x1\": x1, \"x2\": x2, \"x3\": x3}\n",
    "    coordinates = pd.DataFrame(data=coordinates, index=df_input.index)\n",
    "\n",
    "    # Compute Extrinsic (Euclidean) Mean and Project onto Sphere\n",
    "    x_bar = np.mean(coordinates, axis=0)\n",
    "    x_bar = x_bar / np.linalg.norm(x_bar)\n",
    "\n",
    "    # Map to Euclidean Coordinates about the projected extrinsic mean\n",
    "    def Log_Sphere(p):\n",
    "        # Compute dot product between x and p\n",
    "        x_dot_p = np.matmul(x_bar, p)\n",
    "        # Compute Angle Between x and p\n",
    "        x_p_ang = np.arccos(x_dot_p)\n",
    "        # Spherical \"projection\" factor\n",
    "        x_p_fact = x_p_ang / (np.sqrt(1 - (x_dot_p ** 2)))\n",
    "        # Compute Coordinate on Tangent Space\n",
    "        tangent_space_val = (p - x_bar * x_dot_p) * x_p_fact\n",
    "        # Return Ouput\n",
    "        return tangent_space_val\n",
    "\n",
    "    # Return Result\n",
    "    result = [Log_Sphere(row) for row in coordinates.values]\n",
    "    return pd.DataFrame(result, index=df_input.index)\n",
    "\n",
    "\n",
    "# This is our feature map\n",
    "# Function for Joining Euclidean Coordinates with Current DataFrame\n",
    "def feature_map(df_input):\n",
    "    ret_vec = data_to_spherical_to_euclidean(df_input)\n",
    "    df_enriched = pd.concat([df_input, ret_vec], axis=1)\n",
    "    # Reset Index from 1\n",
    "    df_enriched = df_enriched.reset_index(drop=True)\n",
    "    return df_enriched\n",
    "\n",
    "\n",
    "def add_is_train(df, test_size=0.3):\n",
    "    X_train, X_test = train_test_split(\n",
    "        df, shuffle=True, test_size=test_size, random_state=2000\n",
    "    )\n",
    "    X_train = X_train.assign(is_train=True)\n",
    "    X_test = X_test.assign(is_train=False)\n",
    "    X = pd.concat([X_train, X_test])\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "# prepare manual clusters and save them to file\n",
    "def prepare_manual_clusters(source_file, sink_path):\n",
    "    def print_save_messsage(path):\n",
    "        print(str(path) + \" is saved :)\")\n",
    "\n",
    "    df = pd.read_csv(source_file)\n",
    "    df = df.drop([\"total_bedrooms\"], axis=1)\n",
    "\n",
    "    df_bay = df[df.ocean_proximity == \"NEAR BAY\"].drop([\"ocean_proximity\"], axis=1)\n",
    "    df_nocean = df.query(\"ocean_proximity in ['NEAR OCEAN', 'ISLAND']\").drop(\n",
    "        [\"ocean_proximity\"], axis=1\n",
    "    )\n",
    "    df_inland = df[df.ocean_proximity == \"INLAND\"].drop([\"ocean_proximity\"], axis=1)\n",
    "    df_oneHocean = df[df.ocean_proximity == \"<1H OCEAN\"].drop(\n",
    "        [\"ocean_proximity\"], axis=1\n",
    "    )\n",
    "\n",
    "    # Apply Feature Map to each Cut\n",
    "    df_bay = add_is_train(feature_map(df_bay))\n",
    "    df_nocean = add_is_train(feature_map(df_nocean))\n",
    "    df_inland = add_is_train(feature_map(df_inland))\n",
    "    df_oneHocean = add_is_train(feature_map(df_oneHocean))\n",
    "\n",
    "    # Write to CSV\n",
    "    df_bay.to_csv(os.path.join(sink_path, \"bay.csv\"), index=False)\n",
    "    print_save_messsage(os.path.join(sink_path, \"bay.csv\"))\n",
    "    df_nocean.to_csv(os.path.join(sink_path, \"nocean.csv\"), index=False)\n",
    "    print_save_messsage(os.path.join(sink_path, \"nocean.csv\"))\n",
    "    df_inland.to_csv(os.path.join(sink_path, \"inland.csv\"), index=False)\n",
    "    print_save_messsage(os.path.join(sink_path, \"inland.csv\"))\n",
    "    df_oneHocean.to_csv(os.path.join(sink_path, \"oneHocean.csv\"), index=False)\n",
    "    print_save_messsage(os.path.join(sink_path, \"oneHocean.csv\"))\n",
    "\n",
    "    # Prepare Data for FFNNs\n",
    "    # Apply First Feature Map\n",
    "    df_ffNN = pd.concat([df_bay, df_inland, df_nocean, df_oneHocean])\n",
    "    # Map Ocean Proximity to n-ary data\n",
    "    df_housing_complete = pd.get_dummies(df_ffNN)\n",
    "    df_housing_complete.to_csv(\n",
    "        os.path.join(sink_path, \"housing_complete.csv\"), index=False\n",
    "    )\n",
    "    print_save_messsage(os.path.join(sink_path, \"housing_complete.csv\"))\n",
    "\n",
    "\n",
    "def prepare_columntransformer(cl):\n",
    "    min_max = [\n",
    "        \"housing_median_age\",\n",
    "        \"total_rooms\",\n",
    "        \"population\",\n",
    "        \"households\",\n",
    "        \"median_income\",\n",
    "    ]\n",
    "    standard = [\"longitude\", \"latitude\", \"x1\", \"x2\", \"x3\"]\n",
    "    standard_idx = [i for i, obj in enumerate(cl) if obj in standard]\n",
    "    min_max_idx = [i for i, obj in enumerate(cl) if obj in min_max]\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"Min-Max\", MinMaxScaler(), min_max_idx),\n",
    "            (\"zero-one\", StandardScaler(), standard_idx),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    return ct\n",
    "\n",
    "\n",
    "def prepare_data(data_path, manual, test_size=None):\n",
    "    X = pd.read_csv(data_path)\n",
    "    if manual:\n",
    "        X[\"median_house_value\"] = X[\"median_house_value\"] * (10 ** (-5))\n",
    "        X_train = X[X[\"is_train\"] == 1]\n",
    "        X_test = X[X[\"is_train\"] == 0]\n",
    "\n",
    "        y_train = np.array(X_train[\"median_house_value\"])\n",
    "        y_test = np.array(X_test[\"median_house_value\"])\n",
    "\n",
    "        X_train = X_train.drop([\"median_house_value\", \"is_train\"], axis=1)\n",
    "        X_test = X_test.drop([\"median_house_value\", \"is_train\"], axis=1)\n",
    "    else:\n",
    "        y = np.array(X[\"median_house_value\"])\n",
    "        y = y * (10 ** (-5))\n",
    "        X = X.drop([\"median_house_value\"], axis=1)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=2000\n",
    "        )\n",
    "\n",
    "    cl = X_train.columns\n",
    "    X_train = pd.DataFrame(X_train, columns=cl)\n",
    "    X_test = pd.DataFrame(X_test, columns=cl)\n",
    "    \n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss-Function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE, between 0 and 100\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    y_true.shape = (y_true.shape[0], 1)\n",
    "    y_pred.shape = (y_pred.shape[0], 1)\n",
    "\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layer(s)\n",
    "\n",
    "- *fullyConnected_Dense*: are the layers of a Vanilla feed-forward network\n",
    "- *Depth_Selector*: are layers which are used to identify the correct amount of transitive depth to be used..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected_Dense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(fullyConnected_Dense, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "        self.b = self.add_weight(name='bias_ffNN',\n",
    "                                 shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth Selector Layer\n",
    " - No Trainable Parameters besides selection parameter\n",
    " - Sparsely Connected + Trainable Diagonal weights + biases + depth selection parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:*  This layer's parameters are biased to start at the identity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Depth_Selector_sparse_trainable(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units=16, input_dim=32):\n",
    "        super(Depth_Selector_sparse_trainable, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        ### Depth Selection Parameters ###\n",
    "        #--------------------------------#\n",
    "        self.flex_activ1 = self.add_weight(name='flex1',\n",
    "                                          shape=[1],\n",
    "                                          initializer = 'zeros',\n",
    "                                          trainable=True)\n",
    "        self.flex_activ2 = self.add_weight(name='flex2',\n",
    "                                           shape=[1],\n",
    "                                           initializer = 'zeros',\n",
    "                                           trainable=True)\n",
    "        self.flex_activ3 = self.add_weight(name='flex3',\n",
    "                                           shape=[1],\n",
    "                                           initializer = 'zeros',\n",
    "                                           trainable=True)\n",
    "        \n",
    "        \n",
    "        ### Sparsely-Connected Feed-Forward Parameters ###\n",
    "        #------------------------------------------------#\n",
    "        # Trainable Weights #\n",
    "        #-------------------#\n",
    "#         self.w = self.add_weight(shape=(input_shape[-1],),\n",
    "#                                initializer='ones',\n",
    "#                                trainable=True)\n",
    "        self.w = self.add_weight(name='Weights_Mat',\n",
    "                                 shape=(input_shape[-1], self.units),\n",
    "                                 initializer='identity',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='Bias_vects',\n",
    "                                 shape=(input_shape[-1],),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Apply Affine Transform\n",
    "        #-----------------------#\n",
    "        output_spasely_connected_Layer = tf.matmul(inputs, self.w) + self.b\n",
    "        #output_spasely_connected_Layer = tf.math.multiply(inputs, self.w) + self.b\n",
    "        \n",
    "        # Non-Linearity (Activation)\n",
    "        #---------------------------#\n",
    "        # Activation - Extended Parametric ReLU\n",
    "        output_activation = (1+tf.math.pow(self.flex_activ1,2))*tf.nn.relu(output_spasely_connected_Layer) -(1-tf.math.pow(self.flex_activ2,2))*tf.nn.relu(-output_spasely_connected_Layer) + tf.math.pow(self.flex_activ3,2)\n",
    "        \n",
    "        \n",
    "        # Apply Depth Selection Parameter\n",
    "        #--------------------------------#\n",
    "        output_spasely_connected_Layer_selector = output_activation        \n",
    "        \n",
    "        #-#-#\n",
    "        return output_spasely_connected_Layer_selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout Functions\n",
    "#### Modulation Readout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a \"depth-creating-layer\" used for selecting correct amounts of depth!\n",
    "class Modulator_Readout(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Modulator_Readout, self).__init__()\n",
    "        \n",
    "        # Write Internal Variables!\n",
    "        self.lambda1 = self.add_weight(name='Modulation_Parameter',\n",
    "                                       shape=[1],\n",
    "                                       initializer = 'random_uniform',\n",
    "                                       trainable=True)\n",
    "\n",
    "    def call(self,inputs):    \n",
    "        # Define modulating parameter\n",
    "        modulation_param = tf.math.exp(-(self.lambda1))\n",
    "        \n",
    "        # Modulate Output\n",
    "        modified_output = inputs*modulation_param\n",
    "\n",
    "        # Return Output after modulation\n",
    "        return modified_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Activation Function(s)\n",
    "\n",
    "#### Rescaled Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescaled_Leaky_ReLU(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Rescaled_Leaky_ReLU, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.flex_activ = self.add_weight(shape=[1],\n",
    "                        initializer = 'ones',\n",
    "                        trainable=True)\n",
    "        self.flex_activ2 = self.add_weight(shape=[1],\n",
    "                                initializer = 'ones',\n",
    "                                trainable=True)\n",
    "        self.flex_activ3 = self.add_weight(shape=[1],\n",
    "                                initializer = 'zeros',\n",
    "                                trainable=True)\n",
    "        \n",
    "        self.flex_poly_pow = self.add_weight(shape=[1],\n",
    "                                initializer = 'zeros',\n",
    "                                trainable=True)\n",
    "        \n",
    "        self.flex_exp = self.add_weight(shape=[1],\n",
    "                                initializer = 'zeros',\n",
    "                                trainable=True)\n",
    "        \n",
    "\n",
    "    def call(self,inputs):    \n",
    "        # Activation\n",
    "        out_pos = tf.nn.relu(inputs) \n",
    "        out_post = (1+tf.math.abs(self.flex_activ))*out_post + tf.math.pow(out_post,self.flex_poly_pow) + tf.math.exp(tf.math.abs(self.flex_exp)*out_post)\n",
    "        out_neg = -(tf.math.abs(self.flex_activ2))*tf.nn.relu(-inputs)        \n",
    "        out_shift = tf.math.abs(self.flex_activ3)\n",
    "        output_activation = out_pos + out_neg + out_shift\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Architecture (Model)\n",
    "\n",
    "- The parameter *istype* automatically decides on the model to be used:\n",
    "  - *Vanilla feed-forward layers* set: **istype==0**\n",
    "  - *Chaos Reparameterization of Fully-Connected feed-forward layers* set: **istype==1**\n",
    "  \n",
    "- The parameter *ismodulated* automatically decides if the modulator readout should (or should not) be applied:\n",
    "  - *Linear Readout* set: **ismodulated==0**,\n",
    "  - *Modulated Readout* set: **ismodulated==1**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hidden layer neural network\n",
    "def def_model(height, depth, learning_rate, transitive_depth, ismodulated, istype, input_dim, dp):\n",
    "    #--------------------------------------------------#\n",
    "    # Build Regular Arch.\n",
    "    #--------------------------------------------------#\n",
    "    \n",
    "    #-###################-#\n",
    "    # Define Model Input -#\n",
    "    #-###################-#\n",
    "    inputs_ffNN = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #-#########################-#\n",
    "    # Sparse Transitive Layers -#\n",
    "    #-#########################-#\n",
    "    if transitive_depth > 0:\n",
    "        x_ffNN = Depth_Selector_sparse_trainable(input_dim)(inputs_ffNN)\n",
    "        \n",
    "        #-----------------------------#\n",
    "        # Apply Deep Transitive Layers\n",
    "        #-----------------------------#\n",
    "        for j in range((transitive_depth-1)):\n",
    "            x_ffNN = Depth_Selector_sparse_trainable(input_dim)(x_ffNN)\n",
    "            \n",
    "        #---------------------#\n",
    "        # Define Input Layer -#\n",
    "        #---------------------#\n",
    "        if istype == 1: # 1== TCP\n",
    "            x_ffNN = sparselyConnected(height)(x_ffNN)\n",
    "        else: #ffNN Vanilla\n",
    "            x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        #---------------------#\n",
    "        # Define Input Layer -#\n",
    "        #---------------------#\n",
    "        if istype == 1: # 1== TCP\n",
    "            x_ffNN = sparselyConnected(height)(inputs_ffNN)\n",
    "        else: #ffNN Vanilla\n",
    "            x_ffNN = fullyConnected_Dense(height)(inputs_ffNN)\n",
    "\n",
    "    \n",
    "    \n",
    "    #-##############################################################-#\n",
    "    #### - - - (Reparameterization of) Feed-Forward Network - - - ####\n",
    "    #-##############################################################-#\n",
    "    for i in range(depth):\n",
    "        #----------------------#\n",
    "        # Choice of Activation #\n",
    "        #----------------------#\n",
    "        # ReLU Activation\n",
    "        #x_ffNN = tf.nn.relu(x_ffNN)\n",
    "        # Sigmoid Activation\n",
    "        #x_ffNN = tf.math.sigmoid(x_ffNN)\n",
    "        # Leaky ReLU Activation\n",
    "        x_ffNN = Rescaled_Leaky_ReLU()(x_ffNN)\n",
    "        \n",
    "        #-------------#\n",
    "        # Dense Layer #\n",
    "        #-------------#\n",
    "        if istype == 1: # 1== TCP\n",
    "            x_ffNN = sparselyConnected(height)(x_ffNN)\n",
    "        else: #ffNN Vanilla\n",
    "            x_ffNN = fullyConnected_Dense(height)(x_ffNN)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #-####################-#\n",
    "    # Define Readout Layer #\n",
    "    #-####################-#\n",
    "    # Apply Modulation Layer (T/F?)\n",
    "    if ismodulated == 1:\n",
    "            \n",
    "        # Apply Final Layer\n",
    "        if istype == 1: # 1== TCP\n",
    "            x_ffNN = sparselyConnected(1)(x_ffNN)\n",
    "        else: #ffNN Vanilla\n",
    "            x_ffNN = fullyConnected_Dense(1)(x_ffNN)\n",
    "        # Apply Modulation\n",
    "        outputs_ffNN = Modulator_Readout()(x_ffNN)\n",
    "    else:\n",
    "        # Apply Final Layer\n",
    "        if istype == 1: # 1== TCP\n",
    "            outputs_ffNN = sparselyConnected(1)(x_ffNN)\n",
    "        else: #ffNN Vanilla\n",
    "            outputs_ffNN = fullyConnected_Dense(1)(x_ffNN)\n",
    "    \n",
    "    \n",
    "    # Define Model Output\n",
    "    ffNN = tf.keras.Model(inputs_ffNN, outputs_ffNN)\n",
    "    #--------------------------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "\n",
    "    ffNN.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return ffNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & CV Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the structure defined using grid and report the predicted y_train and y_test\n",
    "def fit_structure(X_train, y_train, X_test, grid):\n",
    "\n",
    "    t_start = time()\n",
    "    estimator_cur = grid.fit(X_train, y_train)\n",
    "    t_dur = time() - t_start\n",
    "    best_params = estimator_cur.best_params_\n",
    "\n",
    "    y_test_hat = estimator_cur.predict(X_test)\n",
    "    y_train_hat = estimator_cur.predict(X_train)\n",
    "    K.clear_session()\n",
    "    del estimator_cur\n",
    "    gc.collect()\n",
    "\n",
    "    return best_params, y_test_hat, y_train_hat, t_dur\n",
    "\n",
    "\n",
    "# return mape, mae, mse and me based on y and y_hat\n",
    "def evaluate_structure(y, y_hat):\n",
    "    mape = mean_absolute_percentage_error(y_pred=y_hat, y_true=y)\n",
    "    mae = mean_absolute_error(y_hat, y)\n",
    "    mse = mean_squared_error(y_hat, y)\n",
    "    me = np.mean(y_hat - y)\n",
    "\n",
    "    # N Data Points\n",
    "    size_test = y_hat.shape[0]\n",
    "\n",
    "    return mape, mae, mse, me, size_test\n",
    "\n",
    "\n",
    "def build_grid(param_grid, n_iter, k, n_jobs, cl):\n",
    "\n",
    "    model = KerasRegressor(build_fn=def_model, verbose=2)\n",
    "    ct = prepare_columntransformer(cl)\n",
    "    model = Pipeline([(\"preprocess\", ct), (\"model\", model)])\n",
    "\n",
    "    grid = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        n_jobs=n_jobs,\n",
    "        cv=KFold(k, random_state=2000, shuffle=True),\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=n_iter,\n",
    "        return_train_score=True,\n",
    "        random_state=2000,\n",
    "    )\n",
    "    return grid\n",
    "\n",
    "\n",
    "# evaluate the branching structure and update the report of fit for each branch and total\n",
    "def evaluate_branching_structure(branches, param, k, n_iter, n_jobs):\n",
    "\n",
    "    key_vec = branches.keys()\n",
    "\n",
    "    for key in key_vec:\n",
    "        branch_cur = branches[key]\n",
    "        y_train = branch_cur[\"y_train\"]\n",
    "        y_test = branch_cur[\"y_test\"]\n",
    "\n",
    "        param_cur = param\n",
    "        cl = branch_cur[\"X_train\"].columns\n",
    "        grid = build_grid(\n",
    "            param_grid=param_cur, n_iter=n_iter, k=k, n_jobs=n_jobs, cl=cl\n",
    "        )\n",
    "        best_params, y_test_hat, y_train_hat, t_dur = fit_structure(\n",
    "            branch_cur[\"X_train\"], y_train, branch_cur[\"X_test\"], grid\n",
    "        )\n",
    "\n",
    "        branch_cur[\"best_params\"] = best_params\n",
    "        branch_cur[\"y_test_hat\"] = y_test_hat\n",
    "        branch_cur[\"y_train_hat\"] = y_train_hat\n",
    "        branch_cur[\"y_test\"] = y_test\n",
    "        branch_cur[\"y_train\"] = y_train\n",
    "        branch_cur[\"t_dur\"] = t_dur / 60        \n",
    "\n",
    "        mape_tr, mae_tr, mse_tr, me_tr, size_tr = evaluate_structure(\n",
    "            y=y_train, y_hat=y_train_hat\n",
    "        )\n",
    "        branch_cur[\"e_train_mape\"] = mape_tr\n",
    "        branch_cur[\"e_train_mae\"] = mae_tr\n",
    "        branch_cur[\"e_train_mse\"] = mse_tr\n",
    "        branch_cur[\"e_train_me\"] = me_tr\n",
    "        branch_cur[\"e_train_size\"] = size_tr\n",
    "\n",
    "        mape_test, mae_test, mse_test, me_test, size_test = evaluate_structure(\n",
    "            y=y_test, y_hat=y_test_hat\n",
    "        )\n",
    "        branch_cur[\"e_test_mape\"] = mape_test\n",
    "        branch_cur[\"e_test_mae\"] = mae_test\n",
    "        branch_cur[\"e_test_mse\"] = mse_test\n",
    "        branch_cur[\"e_test_me\"] = me_test\n",
    "        branch_cur[\"e_test_size\"] = size_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation/ Result Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(mydict, main_path, type):\n",
    "    cur_path = os.path.join(main_path, type)\n",
    "    check_path(cur_path)\n",
    "    txt_path = os.path.join(cur_path, type + \".txt\")\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        for k1, v1 in mydict.items():\n",
    "            for k2, v2 in v1.items():\n",
    "                if k2 in [\n",
    "                    \"X_train\",\n",
    "                    \"y_train\",\n",
    "                    \"X_test\",\n",
    "                    \"y_test\",\n",
    "                    \"y_test_hat\",\n",
    "                    \"y_train_hat\",\n",
    "                ]:\n",
    "\n",
    "                    csv_path = str(\n",
    "                        os.path.join(cur_path, str(k1) + \"_\" + str(k2) + \".csv\")\n",
    "                    )\n",
    "                    pd.DataFrame(v2).to_csv(csv_path, index=False)\n",
    "\n",
    "                else:\n",
    "                    f.write(str(k1) + \" >>> \" + str(k2) + \" >>> \" + str(v2) + \"\\n\\n\")\n",
    "    print(\"Results for \" + str(type) + \" is saved!\")\n",
    "\n",
    "\n",
    "def calculate_results(\n",
    "    model_dir, name, folders=[\"bay\", \"inland\", \"oneHocean\", \"nocean\"], is_train=True\n",
    "):\n",
    "    def evaluate_structure(y, y_hat):\n",
    "        mape = mean_absolute_percentage_error(y_pred=y_hat, y_true=y)\n",
    "        mae = mean_absolute_error(y_hat, y)\n",
    "        mse = mean_squared_error(y_hat, y)\n",
    "        me = np.mean(y_hat - y)\n",
    "        sd = np.std(y_hat - y)\n",
    "\n",
    "        return dict(mae=mae, mape=mape, mse=mse)\n",
    "\n",
    "    i = 0\n",
    "    dir_cur = os.path.join(model_dir, folders[i])\n",
    "    y_test_total = pd.read_csv(os.path.join(dir_cur, \"0_y_test.csv\"))[\"0\"].values\n",
    "    y_test_hat_total = pd.read_csv(os.path.join(dir_cur, \"0_y_test_hat.csv\"))[\n",
    "        \"0\"\n",
    "    ].values\n",
    "    y_train_total = pd.read_csv(os.path.join(dir_cur, \"0_y_train.csv\"))[\"0\"].values\n",
    "    y_train_hat_total = pd.read_csv(os.path.join(dir_cur, \"0_y_train_hat.csv\"))[\n",
    "        \"0\"\n",
    "    ].values\n",
    "    P_time = float(\n",
    "        pd.read_csv(os.path.join(dir_cur, folders[i] + \".txt\"))\n",
    "        .iloc[0, 0]\n",
    "        .split(\">>>\")[2]\n",
    "    )\n",
    "    T_time = P_time\n",
    "\n",
    "    if len(folders) > 1:\n",
    "\n",
    "        for i in range(1, len(folders)):\n",
    "            dir_cur = os.path.join(model_dir, folders[i])\n",
    "            try:\n",
    "                y_test_cur = pd.read_csv(os.path.join(dir_cur, \"0_y_test.csv\"))[\n",
    "                    \"0\"\n",
    "                ].values\n",
    "                y_test_hat_cur = pd.read_csv(os.path.join(dir_cur, \"0_y_test_hat.csv\"))[\n",
    "                    \"0\"\n",
    "                ].values\n",
    "                y_train_cur = pd.read_csv(os.path.join(dir_cur, \"0_y_train.csv\"))[\n",
    "                    \"0\"\n",
    "                ].values\n",
    "                y_train_hat_cur = pd.read_csv(\n",
    "                    os.path.join(dir_cur, \"0_y_train_hat.csv\")\n",
    "                )[\"0\"].values\n",
    "                y_test_total = np.concatenate([y_test_total, y_test_cur])\n",
    "                y_test_hat_total = np.concatenate([y_test_hat_total, y_test_hat_cur])\n",
    "                y_train_total = np.concatenate([y_train_total, y_train_cur])\n",
    "                y_train_hat_total = np.concatenate([y_train_hat_total, y_train_hat_cur])\n",
    "                t_cur = float(\n",
    "                    pd.read_csv(os.path.join(dir_cur, folders[i] + \".txt\"))\n",
    "                    .iloc[0, 0]\n",
    "                    .split(\">>>\")[2]\n",
    "                )\n",
    "                P_time = np.maximum(t_cur, P_time)\n",
    "                T_time = t_cur + T_time\n",
    "            except:\n",
    "                print(str(folders[i]) + \"   is fucked up\")\n",
    "\n",
    "    if is_train:\n",
    "        ret = evaluate_structure(y_train_total, y_train_hat_total)\n",
    "    else:\n",
    "        ret = evaluate_structure(y_test_total, y_test_hat_total)\n",
    "    ret[\"P-time\"] = P_time\n",
    "    ret[\"T_time\"] = T_time\n",
    "    return pd.DataFrame(ret, index=[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Alerts and Notifications\n",
    "beep = lambda x: os.system(\"echo -n '\\a';sleep 0.2;\" * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------#\n",
    "#=### Results & Summarizing ###=#\n",
    "#-------------------------------#\n",
    "def reporter(y_train_hat_in,y_test_hat_in,y_train_in,y_test_in):\n",
    "    # Training Performance\n",
    "    Training_performance = np.array([mean_absolute_error(y_train_hat_in,y_train_in),\n",
    "                                mean_squared_error(y_train_hat_in,y_train_in),\n",
    "                                   mean_absolute_percentage_error(y_train_hat_in,y_train_in)])\n",
    "    # Testing Performance\n",
    "    Test_performance = np.array([mean_absolute_error(y_test_hat_in,y_test_in),\n",
    "                                mean_squared_error(y_test_hat_in,y_test_in),\n",
    "                                   mean_absolute_percentage_error(y_test_hat_in,y_test_in)])\n",
    "    # Organize into Dataframe\n",
    "    Performance_dataframe = pd.DataFrame({'train': Training_performance,'test': Test_performance})\n",
    "    Performance_dataframe.index = [\"MAE\",\"MSE\",\"MAPE\"]\n",
    "    # return output\n",
    "    return Performance_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
