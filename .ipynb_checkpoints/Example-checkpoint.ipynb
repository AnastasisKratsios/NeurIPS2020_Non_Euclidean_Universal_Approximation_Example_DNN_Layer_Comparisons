{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example for Paper**: [Non-Euclidean Universal Approximation](https://arxiv.org/abs/2006.02341)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode:\n",
    "Use this to test script before running with \"train_mode\" $\\triangleq$ False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preping\n",
    "\n",
    "We compare three models in this implementation.  Each are feed-forward networks of the same dimensions:\n",
    "- **Good model**: repsects our assumptions\n",
    "- **Bad model**: does not\n",
    "- **Vanilla model**: is a naive feed-forward benchmark\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Alert(s)\n",
    "import smtplib\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# DL: Tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.utils.layer_utils import count_params\n",
    "from tensorflow.python.framework import ops # Custome Tensorflow Functions\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "# DL: Tensorflow - Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Formatting:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "# Random Forest & Gradient Boosting (Arch. Construction)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Structuring\n",
    "from pathlib import Path\n",
    "\n",
    "# Visulatization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Writing, Reading, Exporting, and Importing\n",
    "#from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Misc\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "\n",
    "# Set-Seed\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Externally-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Utility\n",
    "exec(open('Helper_Utility.py').read())\n",
    "# Helper Functions Utility\n",
    "exec(open('Optimal_Deep_Feature_and_Readout_Util.py').read())\n",
    "# Extra Utilities\n",
    "exec(open('Hyperparameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = \"./data/housing_complete.csv\"\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Parse/Prepare Data\n",
    "X_train, y_train, X_test, y_test= prepare_data(data_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and Make Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('./outputs/models/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Vanilla/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Bad/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/Good_I/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/Good_II/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/tables/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/results/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Seed(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed Tensorflow:\n",
    "tf.random.set_seed(2020)\n",
    "# Set seed Numpy:\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $I$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  \n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Mode: <Good I>\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Nice_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Nice_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 1.1159 - mse: 4.5747 - mae: 1.1159 - mape: 66.2293\n",
      "Epoch 2/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.8357 - mse: 1.2112 - mae: 0.8357 - mape: 53.0327\n",
      "Epoch 3/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7877 - mse: 1.0624 - mae: 0.7877 - mape: 50.2913\n",
      "Epoch 4/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7736 - mse: 1.0288 - mae: 0.7736 - mape: 50.0069\n",
      "Epoch 5/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7679 - mse: 1.0062 - mae: 0.7679 - mape: 49.5419\n",
      "Epoch 6/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.7642 - mse: 0.9991 - mae: 0.7642 - mape: 49.5463\n",
      "Epoch 7/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7572 - mse: 0.9814 - mae: 0.7572 - mape: 48.7538\n",
      "Epoch 8/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7542 - mse: 0.9716 - mae: 0.7542 - mape: 48.6152\n",
      "Epoch 9/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.7522 - mse: 0.9738 - mae: 0.7522 - mape: 48.5201\n",
      "Epoch 10/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7429 - mse: 0.9490 - mae: 0.7429 - mape: 47.7641\n",
      "Epoch 11/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7411 - mse: 0.9536 - mae: 0.7411 - mape: 47.9149\n",
      "Epoch 12/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7374 - mse: 0.9543 - mae: 0.7374 - mape: 47.6893\n",
      "Epoch 13/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7329 - mse: 0.9286 - mae: 0.7329 - mape: 47.3481\n",
      "Epoch 14/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.7386 - mse: 0.9442 - mae: 0.7386 - mape: 47.6249\n",
      "Epoch 15/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7277 - mse: 0.9129 - mae: 0.7277 - mape: 47.1259\n",
      "Epoch 16/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7246 - mse: 0.9054 - mae: 0.7246 - mape: 46.6592\n",
      "Epoch 17/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7237 - mse: 0.9035 - mae: 0.7237 - mape: 46.6411\n",
      "Epoch 18/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7234 - mse: 0.9132 - mae: 0.7234 - mape: 46.4159\n",
      "Epoch 19/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7179 - mse: 0.8908 - mae: 0.7179 - mape: 46.2119\n",
      "Epoch 20/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7133 - mse: 0.8699 - mae: 0.7133 - mape: 46.0716\n",
      "Epoch 21/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7116 - mse: 0.8803 - mae: 0.7116 - mape: 45.5251\n",
      "Epoch 22/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7044 - mse: 0.8594 - mae: 0.7044 - mape: 45.3009\n",
      "Epoch 23/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6982 - mse: 0.8476 - mae: 0.6982 - mape: 44.3713\n",
      "Epoch 24/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6861 - mse: 0.8213 - mae: 0.6861 - mape: 43.0038\n",
      "Epoch 25/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6768 - mse: 0.8023 - mae: 0.6768 - mape: 42.0553\n",
      "Epoch 26/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6604 - mse: 0.7722 - mae: 0.6604 - mape: 40.9421\n",
      "Epoch 27/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6421 - mse: 0.7346 - mae: 0.6421 - mape: 39.5548\n",
      "Epoch 28/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.6230 - mse: 0.6957 - mae: 0.6230 - mape: 37.8763\n",
      "Epoch 29/50\n",
      "1806/1806 [==============================] - 6s 3ms/step - loss: 0.6103 - mse: 0.6747 - mae: 0.6103 - mape: 36.7005\n",
      "Epoch 30/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.6024 - mse: 0.6642 - mae: 0.6024 - mape: 36.2614\n",
      "Epoch 31/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.5980 - mse: 0.6566 - mae: 0.5980 - mape: 35.7288\n",
      "Epoch 32/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.5911 - mse: 0.6422 - mae: 0.5911 - mape: 35.2949\n",
      "Epoch 33/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.5895 - mse: 0.6385 - mae: 0.5895 - mape: 35.1545\n",
      "Epoch 34/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5790 - mse: 0.6172 - mae: 0.5790 - mape: 34.3371\n",
      "Epoch 35/50\n",
      "1806/1806 [==============================] - 6s 3ms/step - loss: 0.5738 - mse: 0.6122 - mae: 0.5738 - mape: 33.7475\n",
      "Epoch 36/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.5845 - mse: 0.6309 - mae: 0.5845 - mape: 34.5810\n",
      "Epoch 37/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5780 - mse: 0.6233 - mae: 0.5780 - mape: 33.9283\n",
      "Epoch 38/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.5773 - mse: 0.6189 - mae: 0.5773 - mape: 33.8935\n",
      "Epoch 39/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5735 - mse: 0.6135 - mae: 0.5735 - mape: 33.6522\n",
      "Epoch 40/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5632 - mse: 0.5968 - mae: 0.5632 - mape: 32.8679\n",
      "Epoch 41/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5481 - mse: 0.5726 - mae: 0.5481 - mape: 31.6512\n",
      "Epoch 42/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5545 - mse: 0.5857 - mae: 0.5545 - mape: 32.0524\n",
      "Epoch 43/50\n",
      "1806/1806 [==============================] - 7s 4ms/step - loss: 0.5523 - mse: 0.5743 - mae: 0.5523 - mape: 32.1361\n",
      "Epoch 44/50\n",
      "1806/1806 [==============================] - 7s 4ms/step - loss: 0.5512 - mse: 0.5762 - mae: 0.5512 - mape: 31.9434\n",
      "Epoch 45/50\n",
      "1806/1806 [==============================] - 6s 3ms/step - loss: 0.5521 - mse: 0.5789 - mae: 0.5521 - mape: 31.9300\n",
      "Epoch 46/50\n",
      "1806/1806 [==============================] - 6s 3ms/step - loss: 0.5556 - mse: 0.5757 - mae: 0.5556 - mape: 32.4412\n",
      "Epoch 47/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5405 - mse: 0.5528 - mae: 0.5405 - mape: 31.1868\n",
      "Epoch 48/50\n",
      "1806/1806 [==============================] - 6s 3ms/step - loss: 0.5449 - mse: 0.5629 - mae: 0.5449 - mape: 31.5457\n",
      "Epoch 49/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5414 - mse: 0.5602 - mae: 0.5414 - mape: 31.1132\n",
      "Epoch 50/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.5330 - mse: 0.5428 - mae: 0.5330 - mape: 30.5776\n",
      "1806/1806 [==============================] - 2s 1ms/step\n",
      "775/775 [==============================] - 1s 1ms/step\n",
      "Cross-Validated Model: <Good I>\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_good, y_hat_test_good = build_and_predict_nice_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated Model: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $II$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ (x,\\phi_{\\operatorname{Random}}(x)):\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi_{\\operatorname{Random}}(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$,\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices, and $A_i,b_i$ are generated randomly by drawing their components from the standardized Bernoulli distribution.\n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Deep Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step: 0.25\n",
      "Current Step: 0.5\n",
      "Current Step: 0.75\n",
      "Current Step: 1.0\n",
      "Generated Features: Done!\n",
      "   longitude  latitude  housing_median_age  total_rooms  population  \\\n",
      "0    -122.28     37.90                52.0       2261.0       819.0   \n",
      "1    -122.28     37.75                20.0       1156.0       583.0   \n",
      "2    -122.35     37.93                41.0        268.0       198.0   \n",
      "3    -122.44     37.76                30.0       5089.0      1935.0   \n",
      "4    -122.46     37.76                28.0       1072.0       363.0   \n",
      "\n",
      "   households  median_income        x1        x2        x3  longitude  \\\n",
      "0       335.0         4.9083  0.004823  0.020925  0.097437    -122.28   \n",
      "1       326.0         3.1972 -0.010159  0.017270 -0.051777    -122.28   \n",
      "2        82.0         3.2222 -0.009778  0.087640  0.127816    -122.35   \n",
      "3      1139.0         4.6053 -0.050311  0.171876 -0.040424    -122.44   \n",
      "4       168.0         6.1636 -0.055439  0.191173 -0.040088    -122.46   \n",
      "\n",
      "   latitude  housing_median_age  total_rooms  population  households  \\\n",
      "0     37.90                52.0       2261.0       819.0       335.0   \n",
      "1     37.75                20.0       1156.0       583.0       326.0   \n",
      "2     37.93                41.0        268.0       198.0        82.0   \n",
      "3     37.76                30.0       5089.0      1935.0      1139.0   \n",
      "4     37.76                28.0       1072.0       363.0       168.0   \n",
      "\n",
      "   median_income        x1        x2        x3  \n",
      "0         4.9083  0.004823  0.020925  0.097437  \n",
      "1         3.1972 -0.010159  0.017270 -0.051777  \n",
      "2         3.2222 -0.009778  0.087640  0.127816  \n",
      "3         4.6053 -0.050311  0.171876 -0.040424  \n",
      "4         6.1636 -0.055439  0.191173 -0.040088  \n",
      "   longitude  latitude  housing_median_age  total_rooms  population  \\\n",
      "0    -122.19     37.74                36.0        847.0       567.0   \n",
      "1    -122.49     38.10                43.0       1226.0       491.0   \n",
      "2    -122.26     37.81                34.0       5871.0      2689.0   \n",
      "3    -122.09     37.67                33.0       2431.0      1854.0   \n",
      "4    -122.12     37.65                26.0        162.0        86.0   \n",
      "\n",
      "   households  median_income        x1        x2        x3  longitude  \\\n",
      "0       159.0         1.1765  0.012122 -0.069903 -0.061536    -122.19   \n",
      "1       205.0         4.9286 -0.025284  0.214946  0.301317    -122.49   \n",
      "2      1789.0         2.8406  0.000961 -0.000437  0.007889    -122.26   \n",
      "3       603.0         2.7019  0.031197 -0.168769 -0.130512    -122.09   \n",
      "4        25.0         2.3750  0.021410 -0.140280 -0.150747    -122.12   \n",
      "\n",
      "   latitude  housing_median_age  total_rooms  population  households  \\\n",
      "0     37.74                36.0        847.0       567.0       159.0   \n",
      "1     38.10                43.0       1226.0       491.0       205.0   \n",
      "2     37.81                34.0       5871.0      2689.0      1789.0   \n",
      "3     37.67                33.0       2431.0      1854.0       603.0   \n",
      "4     37.65                26.0        162.0        86.0        25.0   \n",
      "\n",
      "   median_income        x1        x2        x3  \n",
      "0         1.1765  0.012122 -0.069903 -0.061536  \n",
      "1         4.9286 -0.025284  0.214946  0.301317  \n",
      "2         2.8406  0.000961 -0.000437  0.007889  \n",
      "3         2.7019  0.031197 -0.168769 -0.130512  \n",
      "4         2.3750  0.021410 -0.140280 -0.150747  \n"
     ]
    }
   ],
   "source": [
    "### Initialize Parameters\n",
    "#------------------------#\n",
    "# Initialize History\n",
    "Randomized_Depth = np.random.poisson(2)\n",
    "past_val = -1\n",
    "current_position = 0\n",
    "# Initalize Features\n",
    "X_train_features = X_train\n",
    "X_test_features = X_test\n",
    "\n",
    "# Construct Deep Randomized Features\n",
    "#------------------------------------#\n",
    "# Set Seed\n",
    "np.random.seed(2020)\n",
    "\n",
    "\n",
    "# Builds Features\n",
    "for i in range(N_Features):    \n",
    "    # Transformations\n",
    "    #-----------------#\n",
    "    # Build\n",
    "    if Randomized_Depth > 0:\n",
    "        # Note: Write Non-Liearly Transformed Features only if transformation has been applied, only if Depth >0\n",
    "        \n",
    "        # Apply Activation\n",
    "        X_train_features_loop = compositer(X_train_features)\n",
    "        X_test_features_loop = compositer(X_test_features)\n",
    "        # Apply Random Weights\n",
    "        Weights_random = (np.random.binomial(1,.5,(X_train_features_loop.shape[1],X_train_features_loop.shape[1])) - .5)*2 # Generate Random Weights\n",
    "        X_train_features_loop = np.matmul(X_train_features_loop,Weights_random)\n",
    "        X_test_features_loop = np.matmul(X_test_features_loop,Weights_random)\n",
    "        # # Apply Bias\n",
    "        biases_random = (np.random.binomial(1,.5,X_train_features_loop.shape[1]) -.5)*2         # Generate Random Weights and Biases from Recentered Binomial Law\n",
    "        X_train_features_loop = X_train_features_loop + biases_random\n",
    "        X_test_features_loop = X_test_features_loop + biases_random\n",
    "        \n",
    "    else:\n",
    "        X_train_features_loop = X_train_features\n",
    "        X_test_features_loop = X_test_features\n",
    "\n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print(\"Current Step: \" +str((i+1)/N_Features))\n",
    "    \n",
    "# Coerce into nice form:\n",
    "X_train_features = pd.DataFrame(X_train_features)\n",
    "X_test_features = pd.DataFrame(X_test_features)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_train_features.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "X_test_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create Features\n",
    "Random_Feature_Space_train = pd.concat([X_train,X_train_features],axis=1)\n",
    "Random_Feature_Space_test = pd.concat([X_test,X_test_features],axis=1)\n",
    "\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Generated Features: Done!')\n",
    "print(Random_Feature_Space_train.head())\n",
    "print(Random_Feature_Space_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Mode: <Good II>\n"
     ]
    }
   ],
   "source": [
    "# Reload Grid\n",
    "exec(open('Hyperparameter_Grid.py').read())\n",
    "# Adjust Input Space's Dimension\n",
    "param_grid_Nice_Nets['input_dim'] = [Random_Feature_Space_train.shape[1]]\n",
    "\n",
    "def def_trainable_layers_Randomized_Feature(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "#     for i_depth_readout in range(Depth_Readout_Map):\n",
    "#         # First Layer\n",
    "#         if i_depth_readout == 0:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "#         else:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_modelII(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Randomized_Feature, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(Random_Feature_Space_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(Random_Feature_Space_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(Random_Feature_Space_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good II>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 1.2561 - mse: 4.6080 - mae: 1.2561 - mape: 77.6421\n",
      "Epoch 2/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.8787 - mse: 1.6383 - mae: 0.8787 - mape: 54.3480\n",
      "Epoch 3/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.7790 - mse: 1.2714 - mae: 0.7790 - mape: 47.3148\n",
      "Epoch 4/50\n",
      "1806/1806 [==============================] - 2s 944us/step - loss: 0.7294 - mse: 1.0386 - mae: 0.7294 - mape: 43.7822\n",
      "Epoch 5/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.6880 - mse: 0.9066 - mae: 0.6880 - mape: 41.4550\n",
      "Epoch 6/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.6449 - mse: 0.7755 - mae: 0.6449 - mape: 38.8245\n",
      "Epoch 7/50\n",
      "1806/1806 [==============================] - 2s 939us/step - loss: 0.6398 - mse: 0.7561 - mae: 0.6398 - mape: 38.3579\n",
      "Epoch 8/50\n",
      "1806/1806 [==============================] - 2s 940us/step - loss: 0.6234 - mse: 0.7245 - mae: 0.6234 - mape: 37.4014\n",
      "Epoch 9/50\n",
      "1806/1806 [==============================] - 2s 941us/step - loss: 0.6011 - mse: 0.6843 - mae: 0.6011 - mape: 35.3582\n",
      "Epoch 10/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.6119 - mse: 0.7117 - mae: 0.6119 - mape: 35.6856\n",
      "Epoch 11/50\n",
      "1806/1806 [==============================] - 3s 1ms/step - loss: 0.6128 - mse: 0.7085 - mae: 0.6128 - mape: 35.8057\n",
      "Epoch 12/50\n",
      "1806/1806 [==============================] - 3s 1ms/step - loss: 0.5830 - mse: 0.6486 - mae: 0.5830 - mape: 33.7868\n",
      "Epoch 13/50\n",
      "1806/1806 [==============================] - 2s 952us/step - loss: 0.5828 - mse: 0.6445 - mae: 0.5828 - mape: 33.5206\n",
      "Epoch 14/50\n",
      "1806/1806 [==============================] - 2s 999us/step - loss: 0.5881 - mse: 0.6646 - mae: 0.5881 - mape: 33.9324\n",
      "Epoch 15/50\n",
      "1806/1806 [==============================] - 3s 2ms/step - loss: 0.5667 - mse: 0.6246 - mae: 0.5667 - mape: 32.5492\n",
      "Epoch 16/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5757 - mse: 0.6399 - mae: 0.5757 - mape: 32.9988\n",
      "Epoch 17/50\n",
      "1806/1806 [==============================] - 3s 2ms/step - loss: 0.5677 - mse: 0.6241 - mae: 0.5677 - mape: 32.4566\n",
      "Epoch 18/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5752 - mse: 0.6394 - mae: 0.5752 - mape: 32.8112\n",
      "Epoch 19/50\n",
      "1806/1806 [==============================] - 2s 963us/step - loss: 0.5685 - mse: 0.6206 - mae: 0.5685 - mape: 32.4841\n",
      "Epoch 20/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5651 - mse: 0.6196 - mae: 0.5651 - mape: 32.1318\n",
      "Epoch 21/50\n",
      "1806/1806 [==============================] - 3s 1ms/step - loss: 0.5613 - mse: 0.6185 - mae: 0.5613 - mape: 31.8986\n",
      "Epoch 22/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5568 - mse: 0.6105 - mae: 0.5568 - mape: 31.6118\n",
      "Epoch 23/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5545 - mse: 0.5967 - mae: 0.5545 - mape: 31.4820\n",
      "Epoch 24/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5566 - mse: 0.6116 - mae: 0.5566 - mape: 31.2621\n",
      "Epoch 25/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5633 - mse: 0.6149 - mae: 0.5633 - mape: 31.6941\n",
      "Epoch 26/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5597 - mse: 0.6097 - mae: 0.5597 - mape: 31.5755\n",
      "Epoch 27/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5598 - mse: 0.6147 - mae: 0.5598 - mape: 31.6897\n",
      "Epoch 28/50\n",
      "1806/1806 [==============================] - 2s 985us/step - loss: 0.5535 - mse: 0.6064 - mae: 0.5535 - mape: 31.4758\n",
      "Epoch 29/50\n",
      "1806/1806 [==============================] - 2s 946us/step - loss: 0.5535 - mse: 0.6049 - mae: 0.5535 - mape: 31.3685\n",
      "Epoch 30/50\n",
      "1806/1806 [==============================] - 2s 909us/step - loss: 0.5557 - mse: 0.6061 - mae: 0.5557 - mape: 31.5385\n",
      "Epoch 31/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5621 - mse: 0.6176 - mae: 0.5621 - mape: 31.8480\n",
      "Epoch 32/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5641 - mse: 0.6210 - mae: 0.5641 - mape: 32.0449\n",
      "Epoch 33/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5567 - mse: 0.6103 - mae: 0.5567 - mape: 31.4978\n",
      "Epoch 34/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5528 - mse: 0.6043 - mae: 0.5528 - mape: 31.1849\n",
      "Epoch 35/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5545 - mse: 0.5997 - mae: 0.5545 - mape: 31.4154\n",
      "Epoch 36/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5472 - mse: 0.5950 - mae: 0.5472 - mape: 30.9403\n",
      "Epoch 37/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5537 - mse: 0.6119 - mae: 0.5537 - mape: 31.1871\n",
      "Epoch 38/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5454 - mse: 0.5895 - mae: 0.5454 - mape: 30.7161\n",
      "Epoch 39/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5459 - mse: 0.5848 - mae: 0.5459 - mape: 30.9819\n",
      "Epoch 40/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5518 - mse: 0.6031 - mae: 0.5518 - mape: 31.0436\n",
      "Epoch 41/50\n",
      "1806/1806 [==============================] - 2s 913us/step - loss: 0.5488 - mse: 0.5908 - mae: 0.5488 - mape: 31.0212\n",
      "Epoch 42/50\n",
      "1806/1806 [==============================] - 2s 887us/step - loss: 0.5423 - mse: 0.5810 - mae: 0.5423 - mape: 30.7343\n",
      "Epoch 43/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5522 - mse: 0.6007 - mae: 0.5522 - mape: 31.1697\n",
      "Epoch 44/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5559 - mse: 0.6118 - mae: 0.5559 - mape: 31.1963\n",
      "Epoch 45/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5519 - mse: 0.6001 - mae: 0.5519 - mape: 31.2858\n",
      "Epoch 46/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5472 - mse: 0.5876 - mae: 0.5472 - mape: 30.9235\n",
      "Epoch 47/50\n",
      "1806/1806 [==============================] - 3s 2ms/step - loss: 0.5440 - mse: 0.5880 - mae: 0.5440 - mape: 30.6261\n",
      "Epoch 48/50\n",
      "1806/1806 [==============================] - 3s 2ms/step - loss: 0.5461 - mse: 0.5916 - mae: 0.5461 - mape: 30.8447\n",
      "Epoch 49/50\n",
      "1806/1806 [==============================] - 3s 2ms/step - loss: 0.5407 - mse: 0.5800 - mae: 0.5407 - mape: 30.5234\n",
      "Epoch 50/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 0.5454 - mse: 0.5913 - mae: 0.5454 - mape: 30.6425\n",
      "1806/1806 [==============================] - 1s 608us/step\n",
      "775/775 [==============================] - 0s 634us/step\n",
      "Cross-Validated Model: \"Good II\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_goodII, y_hat_test_goodII = build_and_predict_nice_modelII(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated Model: \"Good II\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Benchmark(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload CV Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('Hyperparameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Model:\n",
    "Build and train the *bad* model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  The maps $\\rho$ and $\\phi$ are neither injective nor are they surjective since $x\\mapsto \\operatorname{ReLU}(x)$ is neither injective nor surjective as a map from $\\mathbb{R}^k$ to $\\mathbb{R}^k$; where $m=n$.  \n",
    "\n",
    "*Note*:  The key point here is that the input and output maps are forced to be of the same dimension.  Note that, this also violates the minimal bounds derivated in [this paper](https://arxiv.org/abs/1710.11278) for deep ReLU networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Bad Model\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Bad_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim,output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_bad_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Bad_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Bad_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Bad_Model_CVer = RandomizedSearchCV(estimator=Bad_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Bad_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Bad_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Bad_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Bad Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 2/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 3/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 4/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 5/50\n",
      "1806/1806 [==============================] - 3s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 6/50\n",
      "1806/1806 [==============================] - 3s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 7/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 8/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 9/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 10/50\n",
      "1806/1806 [==============================] - 2s 1ms/step - loss: 2.0736 - mse: 5.6372 - mae: 2.0736 - mape: 100.0000\n",
      "Epoch 11/50\n",
      " 407/1806 [=====>........................] - ETA: 2s - loss: 2.0638 - mse: 5.5620 - mae: 2.0638 - mape: 100.0000"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_bad, y_hat_test_bad = build_and_predict_bad_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark ffNN Model (Vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Vanilla(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_Vanilla_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Vanilla_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Vanilla, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Vanilla_Model_CVer = RandomizedSearchCV(estimator=Vanilla_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Vanilla_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Vanilla_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Vanilla_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_Vanilla, y_hat_test_Vanilla = build_and_predict_Vanilla_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Predictions/ Comparisons\n",
    "Generate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Models #\n",
    "#------------------#\n",
    "# Results with Good I Model\n",
    "Perform_GoodI = reporter(y_hat_train_good,y_hat_test_good,y_train,y_test)\n",
    "# Results with Good II Model\n",
    "Perform_GoodII = reporter(y_hat_train_goodII,y_hat_test_goodII,y_train,y_test)\n",
    "\n",
    "\n",
    "# Benchmark Models Performance #\n",
    "#------------------------------#\n",
    "# Results with Bad Model\n",
    "Perform_Bad = reporter(y_hat_train_bad,y_hat_test_bad,y_train,y_test)\n",
    "# Results Vanilla\n",
    "Perform_Vanilla = reporter(y_hat_train_Vanilla,y_hat_test_Vanilla,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_train = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.train,\n",
    "                    'Good II': Perform_GoodII.train,\n",
    "                    'Bad': Perform_Bad.train,\n",
    "                    'Vanilla': Perform_Vanilla.train})\n",
    "\n",
    "performance_test = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.test,\n",
    "                    'Good II': Perform_GoodII.test,\n",
    "                    'Bad': Perform_Bad.test,\n",
    "                    'Vanilla': Perform_Vanilla.test})\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_train.to_latex('./outputs/results/Performance_train.txt')\n",
    "performance_test.to_latex('./outputs/results/Performance_test.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_train_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_train))\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_test_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Et-Voila!')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('#-------------------#')\n",
    "print(' PERFORMANCE SUMMARY:')\n",
    "print('#-------------------#')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('---------------------')\n",
    "print('Training Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_train)\n",
    "print('-------------------------------------------------------------')\n",
    "print('---------------------')\n",
    "print('Testing Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_test)\n",
    "print('-------------------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('😊😊 Fin 😊😊')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 😊 Fin 😊\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
