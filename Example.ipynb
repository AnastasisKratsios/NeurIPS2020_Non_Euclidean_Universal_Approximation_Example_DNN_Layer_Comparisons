{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Example for Paper**: [Non-Euclidean Universal Approximation](https://arxiv.org/abs/2006.02341)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode:\n",
    "Use this to test script before running with \"train_mode\" $\\triangleq$ False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mode = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preping\n",
    "\n",
    "We compare three models in this implementation.  Each are feed-forward networks of the same dimensions:\n",
    "- **Good model**: repsects our assumptions\n",
    "- **Bad model**: does not\n",
    "- **Vanilla model**: is a naive feed-forward benchmark\n",
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Alert(s)\n",
    "import smtplib\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# DL: Tensorflow\n",
    "import tensorflow as tf\n",
    "from keras.utils.layer_utils import count_params\n",
    "from tensorflow.python.framework import ops # Custome Tensorflow Functions\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "# DL: Tensorflow - Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Formatting:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import expit\n",
    "\n",
    "# Random Forest & Gradient Boosting (Arch. Construction)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Structuring\n",
    "from pathlib import Path\n",
    "\n",
    "# Visulatization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Writing, Reading, Exporting, and Importing\n",
    "#from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Misc\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "\n",
    "# Set-Seed\n",
    "np.random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Externally-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Utility\n",
    "exec(open('Helper_Utility.py').read())\n",
    "# Helper Functions Utility\n",
    "exec(open('Optimal_Deep_Feature_and_Readout_Util.py').read())\n",
    "# Extra Utilities\n",
    "exec(open('Hyperparameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data_path = \"./data/housing_complete.csv\"\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Parse/Prepare Data\n",
    "X_train, y_train, X_test, y_test= prepare_data(data_path, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and Make Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('./outputs/models/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Vanilla/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Benchmarks/Bad/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/Good_I/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/models/Deep_Features/Good_II/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/tables/').mkdir(parents=True, exist_ok=True)\n",
    "Path('./outputs/results/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $I$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  \n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built Mode: <Good I>\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Nice_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense_Invertible(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.leaky_relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Nice_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 1.1159 - mse: 4.5747 - mae: 1.1159 - mape: 66.2293\n",
      "Epoch 2/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.8357 - mse: 1.2112 - mae: 0.8357 - mape: 53.0327\n",
      "Epoch 3/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7877 - mse: 1.0624 - mae: 0.7877 - mape: 50.2913\n",
      "Epoch 4/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7736 - mse: 1.0288 - mae: 0.7736 - mape: 50.0069\n",
      "Epoch 5/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7679 - mse: 1.0062 - mae: 0.7679 - mape: 49.5419\n",
      "Epoch 6/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.7642 - mse: 0.9991 - mae: 0.7642 - mape: 49.5463\n",
      "Epoch 7/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7572 - mse: 0.9814 - mae: 0.7572 - mape: 48.7538\n",
      "Epoch 8/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7542 - mse: 0.9716 - mae: 0.7542 - mape: 48.6152\n",
      "Epoch 9/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.7522 - mse: 0.9738 - mae: 0.7522 - mape: 48.5201\n",
      "Epoch 10/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7429 - mse: 0.9490 - mae: 0.7429 - mape: 47.7641\n",
      "Epoch 11/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7411 - mse: 0.9536 - mae: 0.7411 - mape: 47.9149\n",
      "Epoch 12/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7374 - mse: 0.9543 - mae: 0.7374 - mape: 47.6893\n",
      "Epoch 13/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7329 - mse: 0.9286 - mae: 0.7329 - mape: 47.3481\n",
      "Epoch 14/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.7386 - mse: 0.9442 - mae: 0.7386 - mape: 47.6249\n",
      "Epoch 15/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7277 - mse: 0.9129 - mae: 0.7277 - mape: 47.1259\n",
      "Epoch 16/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7246 - mse: 0.9054 - mae: 0.7246 - mape: 46.6592\n",
      "Epoch 17/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7237 - mse: 0.9035 - mae: 0.7237 - mape: 46.6411\n",
      "Epoch 18/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7234 - mse: 0.9132 - mae: 0.7234 - mape: 46.4159\n",
      "Epoch 19/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7179 - mse: 0.8908 - mae: 0.7179 - mape: 46.2119\n",
      "Epoch 20/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7133 - mse: 0.8699 - mae: 0.7133 - mape: 46.0716\n",
      "Epoch 21/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7116 - mse: 0.8803 - mae: 0.7116 - mape: 45.5251\n",
      "Epoch 22/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.7044 - mse: 0.8594 - mae: 0.7044 - mape: 45.3009\n",
      "Epoch 23/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6982 - mse: 0.8476 - mae: 0.6982 - mape: 44.3713\n",
      "Epoch 24/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6861 - mse: 0.8213 - mae: 0.6861 - mape: 43.0038\n",
      "Epoch 25/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6768 - mse: 0.8023 - mae: 0.6768 - mape: 42.0553\n",
      "Epoch 26/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6604 - mse: 0.7722 - mae: 0.6604 - mape: 40.9421\n",
      "Epoch 27/50\n",
      "1806/1806 [==============================] - 4s 2ms/step - loss: 0.6421 - mse: 0.7346 - mae: 0.6421 - mape: 39.5548\n",
      "Epoch 28/50\n",
      "1806/1806 [==============================] - 5s 3ms/step - loss: 0.6230 - mse: 0.6957 - mae: 0.6230 - mape: 37.8763\n",
      "Epoch 29/50\n",
      " 357/1806 [====>.........................] - ETA: 7s - loss: 0.6054 - mse: 0.6723 - mae: 0.6054 - mape: 34.9459"
     ]
    }
   ],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_good, y_hat_test_good = build_and_predict_nice_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated Model: <Good I>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Model $II$:\n",
    "Build and train the good model:\n",
    "$$\n",
    "\\rho \\circ f\\circ (x,\\phi_{\\operatorname{Random}}(x)):\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{Leaky-ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi_{\\operatorname{Random}}(x) = \\operatorname{Leaky-ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{Leaky-ReLU}\\bullet (\\exp(A_1)x+b_1)$,\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices, and $A_i,b_i$ are generated randomly by drawing their components from the standardized Bernoulli distribution.\n",
    "\n",
    "\n",
    "The matrices $\\exp(A_i)$, and $\\exp(\\tilde{A}_i)$ are therefore invertible since $\\exp$ maps any square matrix into the associated [General Linear Group](https://en.wikipedia.org/wiki/General_linear_group).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Random Deep Feature(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Parameters\n",
    "#------------------------#\n",
    "# Initialize History\n",
    "Randomized_Depth = np.random.poisson(2)\n",
    "past_val = -1\n",
    "current_position = 0\n",
    "# Initalize Features\n",
    "X_train_features = X_train\n",
    "X_test_features = X_test\n",
    "\n",
    "# Construct Deep Randomized Features\n",
    "#------------------------------------#\n",
    "# Set Seed\n",
    "np.random.seed(2020)\n",
    "\n",
    "\n",
    "# Builds Features\n",
    "for i in range(N_Features):    \n",
    "    # Transformations\n",
    "    #-----------------#\n",
    "    # Build\n",
    "    if Randomized_Depth > 0:\n",
    "        # Note: Write Non-Liearly Transformed Features only if transformation has been applied, only if Depth >0\n",
    "        \n",
    "        # Apply Activation\n",
    "        X_train_features_loop = compositer(X_train_features)\n",
    "        X_test_features_loop = compositer(X_test_features)\n",
    "        # Apply Random Weights\n",
    "        Weights_random = (np.random.binomial(1,.5,(X_train_features_loop.shape[1],X_train_features_loop.shape[1])) - .5)*2 # Generate Random Weights\n",
    "        X_train_features_loop = np.matmul(X_train_features_loop,Weights_random)\n",
    "        X_test_features_loop = np.matmul(X_test_features_loop,Weights_random)\n",
    "        # # Apply Bias\n",
    "        biases_random = (np.random.binomial(1,.5,X_train_features_loop.shape[1]) -.5)*2         # Generate Random Weights and Biases from Recentered Binomial Law\n",
    "        X_train_features_loop = X_train_features_loop + biases_random\n",
    "        X_test_features_loop = X_test_features_loop + biases_random\n",
    "        \n",
    "    else:\n",
    "        X_train_features_loop = X_train_features\n",
    "        X_test_features_loop = X_test_features\n",
    "\n",
    "    # Update User #\n",
    "    #-------------#\n",
    "    print(\"Current Step: \" +str((i+1)/N_Features))\n",
    "    \n",
    "# Coerce into nice form:\n",
    "X_train_features = pd.DataFrame(X_train_features)\n",
    "X_test_features = pd.DataFrame(X_test_features)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_train_features.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "X_test_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create Features\n",
    "Random_Feature_Space_train = pd.concat([X_train,X_train_features],axis=1)\n",
    "Random_Feature_Space_test = pd.concat([X_test,X_test_features],axis=1)\n",
    "\n",
    "# Update User #\n",
    "#-------------#\n",
    "print('Generated Features: Done!')\n",
    "print(Random_Feature_Space_train.head())\n",
    "print(Random_Feature_Space_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Grid\n",
    "exec(open('Hyperparameter_Grid.py').read())\n",
    "# Adjust Input Space's Dimension\n",
    "param_grid_Nice_Nets['input_dim'] = [Random_Feature_Space_train.shape[1]]\n",
    "\n",
    "def def_trainable_layers_Randomized_Feature(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "#     for i_depth_readout in range(Depth_Readout_Map):\n",
    "#         # First Layer\n",
    "#         if i_depth_readout == 0:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "#         else:\n",
    "#             output_layers = fullyConnected_Dense_Invertible(output_dim)(output_layers)\n",
    "#             output_layers = tf.nn.leaky_relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_nice_modelII(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Nice_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Randomized_Feature, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Nice_Model_CVer = RandomizedSearchCV(estimator=Nice_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Nice_Model_CVer.fit(Random_Feature_Space_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Nice_Model_CVer.predict(Random_Feature_Space_train)\n",
    "    y_hat_test = Nice_Model_CVer.predict(Random_Feature_Space_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Mode: <Good II>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_goodII, y_hat_test_goodII = build_and_predict_nice_modelII(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated Model: \"Good II\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Benchmark(s)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload CV Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('Hyperparameter_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Model:\n",
    "Build and train the *bad* model:\n",
    "$$\n",
    "\\rho \\circ f\\circ \\phi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n.\n",
    "$$\n",
    " - $f$ is a shallow feed-forward network with ReLU activation.  \n",
    " - Readout: $\\rho(x) = \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_n)x+\\tilde{b}_n)\\circ \\dots \\circ \\operatorname{ReLU}\\bullet (\\exp(\\tilde{A}_1)x+\\tilde{b}_1)$\n",
    " - Feature Map: $\\phi(x) = \\operatorname{ReLU}\\bullet (\\exp(A_n)x+b_n)\\circ \\dots \\circ\\operatorname{ReLU}\\bullet (\\exp(A_1)x+b_1)$\n",
    "\n",
    "where $A_i,\\tilde{A}_j$ are square matrices.  The maps $\\rho$ and $\\phi$ are neither injective nor are they surjective since $x\\mapsto \\operatorname{ReLU}(x)$ is neither injective nor surjective as a map from $\\mathbb{R}^k$ to $\\mathbb{R}^k$; where $m=n$.  \n",
    "\n",
    "*Note*:  The key point here is that the input and output maps are forced to be of the same dimension.  Note that, this also violates the minimal bounds derivated in [this paper](https://arxiv.org/abs/1710.11278) for deep ReLU networks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Bad_Input_Output(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim,output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    # Deep Feature Map #\n",
    "    #------------------#\n",
    "    for i_feature_depth in range(Depth_Feature_Map):\n",
    "        # First Layer\n",
    "        if i_feature_depth == 0:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(input_layer)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "        else:\n",
    "            deep_feature_map = fullyConnected_Dense(input_dim)(deep_feature_map)\n",
    "            deep_feature_map = tf.nn.relu(deep_feature_map)\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(deep_feature_map)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------#   \n",
    "    for i_depth_readout in range(Depth_Readout_Map):\n",
    "        # First Layer\n",
    "        if i_depth_readout == 0:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "        else:\n",
    "            output_layers = fullyConnected_Dense(output_dim)(output_layers)\n",
    "            output_layers = tf.nn.relu(output_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_bad_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Bad_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Bad_Input_Output, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Bad_Model_CVer = RandomizedSearchCV(estimator=Bad_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Bad_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Bad_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Bad_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Bad Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_bad, y_hat_test_bad = build_and_predict_bad_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark ffNN Model (Vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Define Predictive Model                                   #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def def_trainable_layers_Vanilla(height, Depth_Feature_Map, Depth_Readout_Map, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.relu(core_layers)\n",
    "    # Affine Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)\n",
    "    \n",
    "    \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "#                                      Build Predictive Model                                    #\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "\n",
    "def build_and_predict_Vanilla_model(n_folds , n_jobs):\n",
    "\n",
    "    # Deep Feature Network\n",
    "    Vanilla_Model_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=def_trainable_layers_Vanilla, verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    Vanilla_Model_CVer = RandomizedSearchCV(estimator=Vanilla_Model_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_Nice_Nets,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit\n",
    "    Vanilla_Model_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions\n",
    "    y_hat_train = Vanilla_Model_CVer.predict(X_train)\n",
    "    y_hat_test = Vanilla_Model_CVer.predict(X_test)\n",
    "    \n",
    "    # Return Values\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Built Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize & User Updates\n",
    "#--------------------------#\n",
    "y_hat_train_Vanilla, y_hat_test_Vanilla = build_and_predict_Vanilla_model(n_folds = 2, n_jobs = 2)\n",
    "print('Cross-Validated: Vanilla Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Predictions/ Comparisons\n",
    "Generate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Models #\n",
    "#------------------#\n",
    "# Results with Good I Model\n",
    "Perform_GoodI = reporter(y_hat_train_good,y_hat_test_good,y_train,y_test)\n",
    "# Results with Good II Model\n",
    "Perform_GoodII = reporter(y_hat_train_goodII,y_hat_test_goodII,y_train,y_test)\n",
    "\n",
    "\n",
    "# Benchmark Models Performance #\n",
    "#------------------------------#\n",
    "# Results with Bad Model\n",
    "Perform_Bad = reporter(y_hat_train_bad,y_hat_test_bad,y_train,y_test)\n",
    "# Results Vanilla\n",
    "Perform_Vanilla = reporter(y_hat_train_Vanilla,y_hat_test_Vanilla,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Metrics\n",
    "#----------------------#\n",
    "performance_train = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.train,\n",
    "                    'Good II': Perform_GoodII.train,\n",
    "                    'Bad': Perform_Bad.train,\n",
    "                    'Vanilla': Perform_Vanilla.train})\n",
    "\n",
    "performance_test = pd.DataFrame({\n",
    "                    'Good I': Perform_GoodI.test,\n",
    "                    'Good II': Perform_GoodII.test,\n",
    "                    'Bad': Perform_Bad.test,\n",
    "                    'Vanilla': Perform_Vanilla.test})\n",
    "\n",
    "# Write Results\n",
    "#---------------#\n",
    "# LaTeX\n",
    "performance_train.to_latex('./outputs/results/Performance_train.txt')\n",
    "performance_test.to_latex('./outputs/results/Performance_test.txt')\n",
    "# Write to Txt\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_train_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_train))\n",
    "cur_path = os.path.expanduser('./outputs/results/Performance_test_text.txt')\n",
    "with open(cur_path, \"w\") as f:\n",
    "    f.write(str(performance_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Et-Voila!')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('#-------------------#')\n",
    "print(' PERFORMANCE SUMMARY:')\n",
    "print('#-------------------#')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('---------------------')\n",
    "print('Training Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_train)\n",
    "print('-------------------------------------------------------------')\n",
    "print('---------------------')\n",
    "print('Testing Performance')\n",
    "print('---------------------')\n",
    "print('-------------------------------------------------------------')\n",
    "print(performance_test)\n",
    "print('-------------------------------------------------------------')\n",
    "print(' ')\n",
    "print(' ')\n",
    "print('ðŸ˜ŠðŸ˜Š Fin ðŸ˜ŠðŸ˜Š')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ðŸ˜Š Fin ðŸ˜Š\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
